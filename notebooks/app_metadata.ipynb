{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "app_metadata"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas_gbq import read_gbq\n",
        "\n",
        "\n",
        "# Define your BigQuery SQL query\n",
        "query = \"\"\"\n",
        "    SELECT * FROM `web-scraping-2024.top_chart.top_chart` LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "# Load the data into a Pandas DataFrame\n",
        "df = read_gbq(query, project_id='web-scraping-2024')\n",
        "metadata = df['app_store_id'].apply(lambda x: f\"https://apps.apple.com/US/app/id{x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ii399Gxx89E",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1727267761799,
          "user_tz": -120,
          "elapsed": 669,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8dd2a0a7-acd5-4c5e-9766-9179d14c55af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to fetch data from the app store page\n",
        "def fetch_app_store_data(app_store_url):\n",
        "    try:\n",
        "        # Send a request to the app store URL\n",
        "        response = requests.get(app_store_url, allow_redirects=True)\n",
        "\n",
        "        # If redirection occurs, get the final URL\n",
        "        scraping_url_redirect = response.url\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract the required details\n",
        "        title_with_rating = soup.find('h1', class_='product-header__title app-header__title').text.strip() if soup.find('h1', class_='product-header__title app-header__title') else None\n",
        "\n",
        "        # Remove age rating (like \"12+\") from title\n",
        "        title = re.sub(r'\\s*\\d+\\+\\s*$', '', title_with_rating) if title_with_rating else None\n",
        "\n",
        "        subtitle = soup.find('h2', class_='product-header__subtitle app-header__subtitle').text.strip() if soup.find('h2', class_='product-header__subtitle app-header__subtitle') else None\n",
        "        avg_rating = soup.find('figcaption', class_='we-rating-count star-rating__count').text.strip().split(' • ')[0] if soup.find('figcaption', class_='we-rating-count star-rating__count') else None\n",
        "\n",
        "        # Extract number of ratings and remove the word \"Ratings\"\n",
        "        number_of_ratings = (\n",
        "            soup.find('figcaption', class_='we-rating-count star-rating__count').text.strip().split(' • ')[1].replace(' Ratings', '')\n",
        "            if soup.find('figcaption', class_='we-rating-count star-rating__count') else None\n",
        "        )\n",
        "\n",
        "        developer_name = soup.find('dd', class_='information-list__item__definition').text.strip() if soup.find('dd', class_='information-list__item__definition') else None\n",
        "        developer_url = soup.find('h2', class_='product-header__identity app-header__identity').find('a')['href'] if soup.find('h2', class_='product-header__identity app-header__identity') else None\n",
        "\n",
        "        # Extract size from the specific tag by finding the 'Size' <dt>\n",
        "        size = None\n",
        "        size_term = soup.find('dt', string='Size')  # Use 'string' instead of 'text'\n",
        "        if size_term:\n",
        "            size_definition = size_term.find_next_sibling('dd')  # Get the corresponding <dd>\n",
        "            size = size_definition.text.strip() if size_definition else None\n",
        "\n",
        "        return {\n",
        "            \"scraping_url\": app_store_url,\n",
        "            \"scraping_timestamp\": int(datetime.now().timestamp() * 1_000_000),\n",
        "            \"scraping_url_redirect\": scraping_url_redirect,\n",
        "            \"title\": title,\n",
        "            \"subtitle\": subtitle,\n",
        "            \"avg_rating\": float(avg_rating) if avg_rating else None,\n",
        "            \"number_of_ratings\": number_of_ratings,  # Only the numeric part\n",
        "            \"developer_name\": developer_name,\n",
        "            \"developer_url\": developer_url,\n",
        "            \"size\": size\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {app_store_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a DataFrame df with the app_store_url and other relevant columns.\n",
        "scraped_data = []\n",
        "for index, row in df.iterrows():\n",
        "    app_store_url = f\"https://apps.apple.com/us/app/id{row['app_store_id']}\"  # Assuming you build the URL like this\n",
        "\n",
        "    # Fetch app details\n",
        "    app_data = fetch_app_store_data(app_store_url)\n",
        "\n",
        "    if app_data:\n",
        "        scraped_data.append(app_data)\n",
        "\n",
        "    # Sleep to avoid overwhelming the server\n",
        "    time.sleep(1)\n",
        "\n",
        "# Create a DataFrame from the scraped data\n",
        "df_scraped = pd.DataFrame(scraped_data)\n",
        "\n",
        "# Ensure columns are in the specified order\n",
        "df_scraped = df_scraped[[\n",
        "    \"scraping_url\",\n",
        "    \"scraping_timestamp\",\n",
        "    \"scraping_url_redirect\",\n",
        "    \"title\",\n",
        "    \"subtitle\",\n",
        "    \"avg_rating\",\n",
        "    \"number_of_ratings\",\n",
        "    \"developer_name\",\n",
        "    \"developer_url\",\n",
        "    \"size\"\n",
        "]]\n"
      ],
      "metadata": {
        "id": "vnZydXuRzH0F",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1727267789921,
          "user_tz": -120,
          "elapsed": 15510,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.cloud import storage\n",
        "import io\n",
        "\n",
        "# Initialize GCS client\n",
        "client = storage.Client()\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = 'app_metadata_top_chart'\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "# Function to upload Parquet file to GCS\n",
        "def upload_parquet_to_gcs(df, file_name, bucket):\n",
        "    # Create an in-memory bytes buffer to save the parquet file\n",
        "    buffer = io.BytesIO()\n",
        "\n",
        "    # Write the DataFrame to Parquet format in memory\n",
        "    df.to_parquet(buffer, index=False)\n",
        "\n",
        "    # Move the buffer's position to the beginning\n",
        "    buffer.seek(0)\n",
        "\n",
        "    # Create a blob (file object) in the bucket\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Upload the bytes from the buffer to GCS\n",
        "    blob.upload_from_file(buffer, content_type='application/octet-stream')\n",
        "\n",
        "    print(f\"Uploaded {file_name} to GCS bucket {bucket_name}\")\n",
        "\n",
        "# Function to fetch data from the app store page\n",
        "def fetch_app_store_data(app_store_url):\n",
        "    try:\n",
        "        # Send a request to the app store URL\n",
        "        response = requests.get(app_store_url, allow_redirects=True)\n",
        "\n",
        "        # If redirection occurs, get the final URL\n",
        "        scraping_url_redirect = response.url\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract the required details\n",
        "        title_with_rating = soup.find('h1', class_='product-header__title app-header__title').text.strip() if soup.find('h1', class_='product-header__title app-header__title') else None\n",
        "\n",
        "        # Remove age rating (like \"12+\") from title\n",
        "        title = re.sub(r'\\s*\\d+\\+\\s*$', '', title_with_rating) if title_with_rating else None\n",
        "\n",
        "        subtitle = soup.find('h2', class_='product-header__subtitle app-header__subtitle').text.strip() if soup.find('h2', class_='product-header__subtitle app-header__subtitle') else None\n",
        "        avg_rating = soup.find('figcaption', class_='we-rating-count star-rating__count').text.strip().split(' • ')[0] if soup.find('figcaption', class_='we-rating-count star-rating__count') else None\n",
        "\n",
        "        # Extract number of ratings and remove the word \"Ratings\"\n",
        "        number_of_ratings = (\n",
        "            soup.find('figcaption', class_='we-rating-count star-rating__count').text.strip().split(' • ')[1].replace(' Ratings', '')\n",
        "            if soup.find('figcaption', class_='we-rating-count star-rating__count') else None\n",
        "        )\n",
        "\n",
        "        developer_name = soup.find('dd', class_='information-list__item__definition').text.strip() if soup.find('dd', class_='information-list__item__definition') else None\n",
        "        developer_url = soup.find('h2', class_='product-header__identity app-header__identity').find('a')['href'] if soup.find('h2', class_='product-header__identity app-header__identity') else None\n",
        "\n",
        "        # Extract size from the specific tag by finding the 'Size' <dt>\n",
        "        size = None\n",
        "        size_term = soup.find('dt', string='Size')  # Use 'string' instead of 'text'\n",
        "        if size_term:\n",
        "            size_definition = size_term.find_next_sibling('dd')  # Get the corresponding <dd>\n",
        "            size = size_definition.text.strip() if size_definition else None\n",
        "\n",
        "        return {\n",
        "            \"scraping_url\": app_store_url,\n",
        "            \"scraping_timestamp\": int(datetime.now().timestamp() * 1_000_000),\n",
        "            \"scraping_url_redirect\": scraping_url_redirect,\n",
        "            \"title\": title,\n",
        "            \"subtitle\": subtitle,\n",
        "            \"avg_rating\": float(avg_rating) if avg_rating else None,\n",
        "            \"number_of_ratings\": number_of_ratings,  # Only the numeric part\n",
        "            \"developer_name\": developer_name,\n",
        "            \"developer_url\": developer_url,\n",
        "            \"size\": size\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {app_store_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a DataFrame df with the app_store_id and other relevant columns.\n",
        "scraped_data = []\n",
        "for index, row in df.iterrows():\n",
        "    app_store_url = f\"https://apps.apple.com/us/app/id{row['app_store_id']}\"  # Assuming you build the URL like this\n",
        "\n",
        "    # Fetch app details\n",
        "    app_data = fetch_app_store_data(app_store_url)\n",
        "\n",
        "    if app_data:\n",
        "        # Append to the list\n",
        "        scraped_data.append(app_data)\n",
        "\n",
        "        # Create a DataFrame for the single app data\n",
        "        df_app = pd.DataFrame([app_data])\n",
        "\n",
        "        # Define the file path using app_store_id and scraping date\n",
        "        app_id = row['app_store_id']\n",
        "        scraping_date = datetime.now().strftime('%Y_%m_%d')\n",
        "        file_name = f\"{scraping_date}__{app_id}.parquet\"\n",
        "\n",
        "        # Upload the DataFrame to GCS\n",
        "        upload_parquet_to_gcs(df_app, file_name, bucket)\n",
        "\n",
        "    # Sleep to avoid overwhelming the server\n",
        "    time.sleep(1)\n",
        "\n",
        "# Optionally, create a DataFrame from all scraped data if needed\n",
        "if scraped_data:\n",
        "    df_scraped = pd.DataFrame(scraped_data)\n",
        "\n",
        "    # Ensure columns are in the specified order\n",
        "    df_scraped = df_scraped[[\n",
        "        \"scraping_url\",\n",
        "        \"scraping_timestamp\",\n",
        "        \"scraping_url_redirect\",\n",
        "        \"title\",\n",
        "        \"subtitle\",\n",
        "        \"avg_rating\",\n",
        "        \"number_of_ratings\",\n",
        "        \"developer_name\",\n",
        "        \"developer_url\",\n",
        "        \"size\"\n",
        "    ]]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vXzkS1X7_To",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1727267848920,
          "user_tz": -120,
          "elapsed": 20085,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "82f49d24-4e22-4898-d7b6-de634aafabd3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded 2024_09_25__6446901002.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__6448311069.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__1641486558.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__284815942.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__1542571008.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__835599320.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__310633997.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__1500855883.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__389801252.parquet to GCS bucket app_metadata_top_chart\n",
            "Uploaded 2024_09_25__878577184.parquet to GCS bucket app_metadata_top_chart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery, storage\n",
        "\n",
        "# Initialize clients\n",
        "gcs_client = storage.Client()\n",
        "bq_client = bigquery.Client()\n",
        "\n",
        "# Define your parameters\n",
        "project_id = \"web-scraping-2024\"\n",
        "dataset_id = \"app_metadata\"\n",
        "bucket_name = \"app_metadata_top_chart\"\n",
        "app_metadata_dir_prefix = \"\"\n",
        "\n",
        "def list_parquet_files(bucket_name, prefix):\n",
        "    \"\"\"List all parquet files in a GCS bucket with a given prefix.\"\"\"\n",
        "    bucket = gcs_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=prefix)\n",
        "    return [blob.name for blob in blobs if blob.name.endswith('.parquet')]\n",
        "\n",
        "# Load all parquet files\n",
        "app_metadata_details_files = list_parquet_files(bucket_name, app_metadata_dir_prefix)\n",
        "\n",
        "if not app_metadata_details_files:\n",
        "    print(\"No files found.\")\n",
        "else:\n",
        "    app_metadata_table_ref = bq_client.dataset(dataset_id).table(\"app_metadata\")\n",
        "    app_metadata_job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.PARQUET,  # Parquet file format\n",
        "        autodetect=True\n",
        "    )\n",
        "\n",
        "    for file_name in app_metadata_details_files:\n",
        "        # Start the load job for each file\n",
        "        load_job = bq_client.load_table_from_uri(\n",
        "            f\"gs://{bucket_name}/{file_name}\",\n",
        "            app_metadata_table_ref,\n",
        "            job_config=app_metadata_job_config\n",
        "        )\n",
        "\n",
        "        # Wait for the job to complete\n",
        "        load_job.result()\n",
        "\n",
        "        # Check the result for each file\n",
        "        app_metadata_details_table = bq_client.get_table(app_metadata_table_ref)\n",
        "        print(f\"Loaded {app_metadata_details_table.num_rows} rows from {file_name} into {dataset_id}:app_metadata.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgPJn06XMHLb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1727267930871,
          "user_tz": -120,
          "elapsed": 35342,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fa95d21d-0138-4210-fbd2-c1c4e8e176f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 rows from 2024_09_25__1500855883.parquet into app_metadata:app_metadata.\n",
            "Loaded 2 rows from 2024_09_25__1542571008.parquet into app_metadata:app_metadata.\n",
            "Loaded 3 rows from 2024_09_25__1641486558.parquet into app_metadata:app_metadata.\n",
            "Loaded 4 rows from 2024_09_25__284815942.parquet into app_metadata:app_metadata.\n",
            "Loaded 5 rows from 2024_09_25__310633997.parquet into app_metadata:app_metadata.\n",
            "Loaded 6 rows from 2024_09_25__389801252.parquet into app_metadata:app_metadata.\n",
            "Loaded 7 rows from 2024_09_25__6446901002.parquet into app_metadata:app_metadata.\n",
            "Loaded 8 rows from 2024_09_25__6448311069.parquet into app_metadata:app_metadata.\n",
            "Loaded 9 rows from 2024_09_25__835599320.parquet into app_metadata:app_metadata.\n",
            "Loaded 10 rows from 2024_09_25__878577184.parquet into app_metadata:app_metadata.\n"
          ]
        }
      ]
    }
  ]
}