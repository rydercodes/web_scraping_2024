{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium\n",
    "%pip install webdriver-manager\n",
    "%pip install bs4\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, TimeoutException\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from google.cloud import logging as cloud_logging\n",
    "\n",
    "# Set up Google Cloud Logging\n",
    "cloud_client = cloud_logging.Client()\n",
    "cloud_client.setup_logging()\n",
    "\n",
    "# Create a custom logger for Google Cloud\n",
    "logger = logging.getLogger('ApptweakScraper')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# ScrapeOps API configuration\n",
    "SCRAPEOPS_API_KEY = '4704d4b5-7bd5-4ffb-97f8-7f12a46a55b6'\n",
    "SCRAPEOPS_PROXY_URL = 'https://proxy.scrapeops.io/v1/'\n",
    "\n",
    "def get_user_agent_list():\n",
    "    response = requests.get(f'http://headers.scrapeops.io/v1/user-agents?api_key={SCRAPEOPS_API_KEY}')\n",
    "    json_response = response.json()\n",
    "    return json_response.get('result', [])\n",
    "\n",
    "def get_random_user_agent(user_agent_list):\n",
    "    random_index = randint(0, len(user_agent_list) - 1)\n",
    "    return user_agent_list[random_index]\n",
    "\n",
    "def get_scrapeops_proxy_url():\n",
    "    return f\"{SCRAPEOPS_PROXY_URL}?api_key={SCRAPEOPS_API_KEY}\"\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Retrieve User-Agent List From ScrapeOps\n",
    "user_agent_list = get_user_agent_list()\n",
    "chrome_options.add_argument(f'user-agent={get_random_user_agent(user_agent_list)}')\n",
    "\n",
    "# ScrapeOps Proxy Configuration\n",
    "scrapeops_proxy = get_scrapeops_proxy_url()\n",
    "chrome_options.add_argument(f'--proxy-server={scrapeops_proxy}')\n",
    "\n",
    "# Function to generate Parquet filename\n",
    "def generate_parquet_filename(search):\n",
    "    date_string = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    clean_search = re.sub(r'[^a-zA-Z0-9]', '', search).lower()\n",
    "    now = datetime.utcnow()\n",
    "    epoch = datetime(1970, 1, 1)\n",
    "    seconds_since_epoch = str(int((now - epoch).total_seconds()))\n",
    "    parquet_filename = f'{date_string}__{seconds_since_epoch}__{clean_search}.parquet'\n",
    "    return parquet_filename\n",
    "\n",
    "# Function to upload the Parquet file to GCS\n",
    "def upload_to_gcs(dataframe, bucket_name, folder_name, file_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(f\"{folder_name}/{file_name}\")\n",
    "\n",
    "    temp_file = f\"/tmp/{file_name}\"\n",
    "    dataframe.to_parquet(temp_file, index=False)\n",
    "    blob.upload_from_filename(temp_file)\n",
    "    os.remove(temp_file)\n",
    "    logger.info(f\"File '{file_name}' uploaded to GCS bucket '{bucket_name}' in folder '{folder_name}'.\")\n",
    "\n",
    "# Function to download specific Parquet files from GCS based on the search character\n",
    "def download_specific_parquets(bucket_name, folder_name, search_character):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    parquet_files = [file.name for file in bucket.list_blobs(prefix=folder_name) if file.name.endswith(f'__{search_character}.parquet')]\n",
    "\n",
    "    existing_dfs = []\n",
    "    for file_name in parquet_files:\n",
    "        local_file = f\"/tmp/{os.path.basename(file_name)}\"\n",
    "        blob = bucket.blob(file_name)\n",
    "        blob.download_to_filename(local_file)\n",
    "        df = pd.read_parquet(local_file)\n",
    "        existing_dfs.append(df)\n",
    "        os.remove(local_file)  # Clean up local file\n",
    "\n",
    "    if existing_dfs:\n",
    "        combined_df = pd.concat(existing_dfs, ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "    return combined_df, parquet_files\n",
    "\n",
    "# Function to delete old Parquet files from GCS\n",
    "def delete_parquet_files_from_gcs(bucket_name, folder_name, parquet_files):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    for file_name in parquet_files:\n",
    "        blob = bucket.blob(file_name)\n",
    "        blob.delete()\n",
    "        logger.info(f\"Deleted file: {file_name} from GCS.\")\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Open the website\n",
    "url = 'https://www.apptweak.com/en/free-tools/keyword-auto-suggestions'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "wait = WebDriverWait(driver, 7200)\n",
    "\n",
    "# Get today's date in format YYYY_MM_DD\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "\n",
    "# Initialize GCS bucket and folder information\n",
    "bucket_name = 'apptweak_scraper'\n",
    "batch_folder_name = 'first_batch_of_searches'\n",
    "\n",
    "# Iterate over all letters (e.g., 'a' to 'z', '0' to '9')\n",
    "for search in 'b':  # You can loop through 'abcdefghijklmnopqrstuvwxyz0123456789'\n",
    "    # Download existing data from GCS for the specific search character\n",
    "    existing_data, parquet_files = download_specific_parquets(bucket_name, batch_folder_name + '/' + today_date, search)\n",
    "\n",
    "    master_df = pd.DataFrame(columns=['search', 'store', 'country', 'language', 'next_word_letter', 'rank', 'suggestion', 'scraping_url', 'scraping_timestamp'])\n",
    "\n",
    "    # Retrieve available options for stores and countries\n",
    "    store_options = get_dropdown_options('store')\n",
    "    country_options = get_dropdown_options('country')[2:50]\n",
    "\n",
    "    # Function to select options from a dropdown\n",
    "    def select_from_dropdown(dropdown_id, option_text):\n",
    "        select = Select(wait.until(EC.presence_of_element_located((By.ID, dropdown_id))))\n",
    "        try:\n",
    "            select.select_by_visible_text(option_text)\n",
    "            logger.info(f\"Selected '{option_text}' from dropdown '{dropdown_id}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error selecting '{option_text}' from dropdown '{dropdown_id}': {e}\")\n",
    "\n",
    "    # Function to get options from a dropdown\n",
    "    def get_dropdown_options(dropdown_id):\n",
    "        select = Select(wait.until(EC.presence_of_element_located((By.ID, dropdown_id))))\n",
    "        options = [option.text for option in select.options]\n",
    "        return options\n",
    "\n",
    "    # Function to get available languages for a specific country\n",
    "    def get_available_languages_for_country(country_name):\n",
    "        select_from_dropdown('country', country_name)\n",
    "        time.sleep(1)  # Wait for the language dropdown to update based on the selected country\n",
    "        language_options = get_dropdown_options('language')\n",
    "        return language_options\n",
    "\n",
    "    # Iterate over all stores\n",
    "    for store_name in store_options:\n",
    "        # Iterate over all countries\n",
    "        for country_name in country_options:\n",
    "            # Get available languages for the current country\n",
    "            language_options = get_available_languages_for_country(country_name)\n",
    "\n",
    "            # Iterate over all languages available for the current country\n",
    "            for language_name in language_options:\n",
    "                logger.info(f\"Processing: Store='{store_name}', Country='{country_name}', Language='{language_name}'\")\n",
    "\n",
    "                # Wait for the 'keyword' input field to be available and enter the letter or digit\n",
    "                keyword_input = wait.until(EC.presence_of_element_located((By.ID, 'keyword')))\n",
    "                keyword_input.clear()\n",
    "                keyword_input.send_keys(search)\n",
    "                keyword_input.send_keys(Keys.RETURN)\n",
    "\n",
    "                # Select the store, country, and language\n",
    "                select_from_dropdown('store', store_name)\n",
    "                select_from_dropdown('country', country_name)\n",
    "                select_from_dropdown('language', language_name)\n",
    "\n",
    "                # Wait for the progress bar to disappear\n",
    "                try:\n",
    "                    wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, '.progress-bar__inner')))\n",
    "                except TimeoutException:\n",
    "                    logger.warning(f\"Progress bar did not disappear within 60 seconds for search '{search}'.\")\n",
    "\n",
    "                # Retry clicking the \"Suggest\" button\n",
    "                suggest_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.js-keyword-auto-suggestion')))\n",
    "                for _ in range(3):\n",
    "                    try:\n",
    "                        suggest_button.click()\n",
    "                        break\n",
    "                    except ElementClickInterceptedException:\n",
    "                        time.sleep(2)\n",
    "                        logger.info(f\"Retrive data by clicking 'Suggest' button for search '{search}'.\")\n",
    "\n",
    "                # Wait for search results section\n",
    "                results_section = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.free-tools__kw-lists-container')))\n",
    "\n",
    "                # Select all <ul> elements with id starting with 'result_'\n",
    "                ul_elements = driver.find_elements(By.XPATH, \"//ul[starts-with(@id, 'result_')]\")\n",
    "\n",
    "                # Process each <ul> and extract its <li> elements\n",
    "                data = []\n",
    "                for index, ul in enumerate(ul_elements):\n",
    "                    li_elements = ul.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "                    # Generate the header based on the index\n",
    "                    next_word_letter = chr(96 + index) if index != 0 else None\n",
    "\n",
    "                    if li_elements:\n",
    "                        for rank, li in enumerate(li_elements[1:], start=1):  # Ignore the first <li>\n",
    "                            suggestion = li.text.strip()\n",
    "\n",
    "                            # Append data for each suggestion\n",
    "                            data.append([search, store_name, country_name, language_name, next_word_letter, rank, suggestion, url, datetime.now().strftime('%Y_%m_%d %H:%M:%S')])\n",
    "\n",
    "                # Append new data to master DataFrame\n",
    "                master_df = pd.concat([master_df, pd.DataFrame(data, columns=['search', 'store', 'country', 'language', 'next_word_letter', 'rank', 'suggestion', 'scraping_url', 'scraping_timestamp'])], ignore_index=True)\n",
    "\n",
    "    # Combine existing data with new data and remove duplicates\n",
    "    combined_df = pd.concat([existing_data, master_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Delete the specific Parquet files from GCS after uploading the new combined data\n",
    "    delete_parquet_files_from_gcs(bucket_name, batch_folder_name + '/' + today_date, parquet_files)\n",
    "\n",
    "    # Save the combined DataFrame to Parquet and upload to GCS\n",
    "    parquet_filename = generate_parquet_filename(search)\n",
    "    upload_to_gcs(combined_df, bucket_name, batch_folder_name + '/' + today_date, parquet_filename)\n",
    "\n",
    "# Clean up\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "\n",
    "# Initialize clients\n",
    "gcs_client = storage.Client()\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "# Define your parameters\n",
    "project_id = \"web-scraping-2024\"\n",
    "dataset_id = \"autosuggest\"\n",
    "bucket_name = \"apptweak_scraper\"\n",
    "apptweak_dir_prefix = \"first_batch_of_searches/2024_09_23/\"\n",
    "specific_file_name = \"2024_09_23__1727111422__b.parquet\"\n",
    "\n",
    "def list_parquet_files(bucket_name, prefix, specific_file):\n",
    "    \"\"\"List specific parquet file in a GCS bucket and prefix.\"\"\"\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    return [blob.name for blob in blobs if blob.name.endswith('.parquet') and specific_file in blob.name]\n",
    "\n",
    "# Load the specific file\n",
    "apptweak_details_files = list_parquet_files(bucket_name, apptweak_dir_prefix, specific_file_name)\n",
    "\n",
    "if not apptweak_details_files:\n",
    "    print(\"No files found.\")\n",
    "else:\n",
    "    apptweak_table_ref = bq_client.dataset(dataset_id).table(\"apptweak_first_batch\")\n",
    "    apptweak_job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.PARQUET,  # Parquet file format\n",
    "        autodetect=True\n",
    "    )\n",
    "\n",
    "    # Start the load job for the specific file\n",
    "    load_job = bq_client.load_table_from_uri(\n",
    "        f\"gs://{bucket_name}/{apptweak_details_files[0]}\",\n",
    "        apptweak_table_ref,\n",
    "        job_config=apptweak_job_config\n",
    "    )\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    load_job.result()\n",
    "\n",
    "    # Check the result\n",
    "    apptweak_details_table = bq_client.get_table(apptweak_table_ref)\n",
    "    print(f\"Loaded {apptweak_details_table.num_rows} rows into {dataset_id}:apptweak_first_batch.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
